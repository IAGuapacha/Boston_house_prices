{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wbepu01LHJoL"
   },
   "source": [
    "# Trabajo Preparacion, visualizacion de Datos y Machine learning con Python<a class=\"tocSkip\">\n",
    "## Ciencia de datos en Produccion <a class=\"tocSkip\">\n",
    "\n",
    "**Estudiante:** Maria Alejandra Caicedo Chaves\n",
    "\n",
    "**ID:** 1016110796\n",
    "\n",
    "**Email:** macaicedoc_1@uqvirtual.edu.co\n",
    "\n",
    "**Estudiante:** Iván Andrés Guapacha Barrero\n",
    "\n",
    "**ID:** \n",
    "\n",
    "**Email:** iaguapachab@uqvirtual.edu.co\n",
    " \n",
    "**Docente:** [Jose R. Zapata](https://joserzapata.github.io)\n",
    "- https://joserzapata.github.io\n",
    "- https://twitter.com/joserzapata\n",
    "- https://www.linkedin.com/in/jose-ricardo-zapata-gonzalez/       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_RCEnM3HJoL",
    "toc": true
   },
   "source": [
    "<h1>Contenido<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Definir-el-Problema-a-Resolver\" data-toc-modified-id=\"Definir-el-Problema-a-Resolver-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Definir el Problema a Resolver</a></span><ul class=\"toc-item\"><li><span><a href=\"#Describir-los-datos-de-entrada-y-salida\" data-toc-modified-id=\"Describir-los-datos-de-entrada-y-salida-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Describir los datos de entrada y salida</a></span></li></ul></li><li><span><a href=\"#Importar-Librerias\" data-toc-modified-id=\"Importar-Librerias-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Importar Librerias</a></span></li><li><span><a href=\"#Cargar-Datasets\" data-toc-modified-id=\"Cargar-Datasets-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Cargar Datasets</a></span></li><li><span><a href=\"#Descripcion--y-Limpieza-de-los-datos\" data-toc-modified-id=\"Descripcion--y-Limpieza-de-los-datos-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Descripcion  y Limpieza de los datos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Identificacion-de-Variables\" data-toc-modified-id=\"Identificacion-de-Variables-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Identificacion de Variables</a></span></li><li><span><a href=\"#Analisis-General-Univariable-y-Bivariable\" data-toc-modified-id=\"Analisis-General-Univariable-y-Bivariable-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Analisis General Univariable y Bivariable</a></span></li><li><span><a href=\"#Eliminar-columnas-de-datos-Innecesarios\" data-toc-modified-id=\"Eliminar-columnas-de-datos-Innecesarios-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Eliminar columnas de datos Innecesarios</a></span></li><li><span><a href=\"#Remover-Datos-Duplicados-Exactos\" data-toc-modified-id=\"Remover-Datos-Duplicados-Exactos-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Remover Datos Duplicados Exactos</a></span></li><li><span><a href=\"#Procesamiento-de-Datos-Faltantes\" data-toc-modified-id=\"Procesamiento-de-Datos-Faltantes-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Procesamiento de Datos Faltantes</a></span><ul class=\"toc-item\"><li><span><a href=\"#Borrar-Filas\" data-toc-modified-id=\"Borrar-Filas-4.5.1\"><span class=\"toc-item-num\">4.5.1&nbsp;&nbsp;</span>Borrar Filas</a></span></li><li><span><a href=\"#Reemplazar-datos-faltantes-con-la-Media/-Moda/-Mediana-(Mean/-Mode/-Median-Imputation)\" data-toc-modified-id=\"Reemplazar-datos-faltantes-con-la-Media/-Moda/-Mediana-(Mean/-Mode/-Median-Imputation)-4.5.2\"><span class=\"toc-item-num\">4.5.2&nbsp;&nbsp;</span>Reemplazar datos faltantes con la Media/ Moda/ Mediana (Mean/ Mode/ Median Imputation)</a></span></li></ul></li><li><span><a href=\"#Remover-Datos-Duplicados-Exactos\" data-toc-modified-id=\"Remover-Datos-Duplicados-Exactos-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Remover Datos Duplicados Exactos</a></span></li><li><span><a href=\"#Analisis-Univariable\" data-toc-modified-id=\"Analisis-Univariable-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>Analisis Univariable</a></span><ul class=\"toc-item\"><li><span><a href=\"#Variables-Numericas\" data-toc-modified-id=\"Variables-Numericas-4.7.1\"><span class=\"toc-item-num\">4.7.1&nbsp;&nbsp;</span>Variables Numericas</a></span></li><li><span><a href=\"#Variables-Categoricas\" data-toc-modified-id=\"Variables-Categoricas-4.7.2\"><span class=\"toc-item-num\">4.7.2&nbsp;&nbsp;</span>Variables Categoricas</a></span></li></ul></li><li><span><a href=\"#Analisis-Bivariable\" data-toc-modified-id=\"Analisis-Bivariable-4.8\"><span class=\"toc-item-num\">4.8&nbsp;&nbsp;</span>Analisis Bivariable</a></span><ul class=\"toc-item\"><li><span><a href=\"#Numericas-vs-Numericas\" data-toc-modified-id=\"Numericas-vs-Numericas-4.8.1\"><span class=\"toc-item-num\">4.8.1&nbsp;&nbsp;</span>Numericas vs Numericas</a></span></li><li><span><a href=\"#Categoricas-vs-Categoricas\" data-toc-modified-id=\"Categoricas-vs-Categoricas-4.8.2\"><span class=\"toc-item-num\">4.8.2&nbsp;&nbsp;</span>Categoricas vs Categoricas</a></span></li><li><span><a href=\"#Categoricas-vs-Numericas\" data-toc-modified-id=\"Categoricas-vs-Numericas-4.8.3\"><span class=\"toc-item-num\">4.8.3&nbsp;&nbsp;</span>Categoricas vs Numericas</a></span></li></ul></li><li><span><a href=\"#Procesamiento-de-Outliers\" data-toc-modified-id=\"Procesamiento-de-Outliers-4.9\"><span class=\"toc-item-num\">4.9&nbsp;&nbsp;</span>Procesamiento de Outliers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Deteccion-de-Outliers-(Univariables-y-Bi-variables)\" data-toc-modified-id=\"Deteccion-de-Outliers-(Univariables-y-Bi-variables)-4.9.1\"><span class=\"toc-item-num\">4.9.1&nbsp;&nbsp;</span>Deteccion de Outliers (Univariables y Bi variables)</a></span></li><li><span><a href=\"#Remover-Outliers\" data-toc-modified-id=\"Remover-Outliers-4.9.2\"><span class=\"toc-item-num\">4.9.2&nbsp;&nbsp;</span>Remover Outliers</a></span></li></ul></li><li><span><a href=\"#Procesamiento-de-Outliers\" data-toc-modified-id=\"Procesamiento-de-Outliers-4.10\"><span class=\"toc-item-num\">4.10&nbsp;&nbsp;</span>Procesamiento de Outliers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Deteccion-de-Outliers-(Univariables-y-Bi-variables)\" data-toc-modified-id=\"Deteccion-de-Outliers-(Univariables-y-Bi-variables)-4.10.1\"><span class=\"toc-item-num\">4.10.1&nbsp;&nbsp;</span>Deteccion de Outliers (Univariables y Bi variables)</a></span></li><li><span><a href=\"#Remover-Outliers\" data-toc-modified-id=\"Remover-Outliers-4.10.2\"><span class=\"toc-item-num\">4.10.2&nbsp;&nbsp;</span>Remover Outliers</a></span></li></ul></li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-4.11\"><span class=\"toc-item-num\">4.11&nbsp;&nbsp;</span>Feature Engineering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Transformacion-de-Variables\" data-toc-modified-id=\"Transformacion-de-Variables-4.11.1\"><span class=\"toc-item-num\">4.11.1&nbsp;&nbsp;</span>Transformacion de Variables</a></span><ul class=\"toc-item\"><li><span><a href=\"#Normalizacion\" data-toc-modified-id=\"Normalizacion-4.11.1.1\"><span class=\"toc-item-num\">4.11.1.1&nbsp;&nbsp;</span>Normalizacion</a></span></li><li><span><a href=\"#Escalamiento\" data-toc-modified-id=\"Escalamiento-4.11.1.2\"><span class=\"toc-item-num\">4.11.1.2&nbsp;&nbsp;</span>Escalamiento</a></span></li><li><span><a href=\"#Logaritmica\" data-toc-modified-id=\"Logaritmica-4.11.1.3\"><span class=\"toc-item-num\">4.11.1.3&nbsp;&nbsp;</span>Logaritmica</a></span></li><li><span><a href=\"#raíz-cuadrada-/-cúbica\" data-toc-modified-id=\"raíz-cuadrada-/-cúbica-4.11.1.4\"><span class=\"toc-item-num\">4.11.1.4&nbsp;&nbsp;</span>raíz cuadrada / cúbica</a></span></li><li><span><a href=\"#Binning-,-Cambios-de-Numericas-a-Categoricas\" data-toc-modified-id=\"Binning-,-Cambios-de-Numericas-a-Categoricas-4.11.1.5\"><span class=\"toc-item-num\">4.11.1.5&nbsp;&nbsp;</span>Binning , Cambios de Numericas a Categoricas</a></span></li></ul></li></ul></li><li><span><a href=\"#Analisis-Univariable-y-Bivariable-Final\" data-toc-modified-id=\"Analisis-Univariable-y-Bivariable-Final-4.12\"><span class=\"toc-item-num\">4.12&nbsp;&nbsp;</span>Analisis Univariable y Bivariable Final</a></span><ul class=\"toc-item\"><li><span><a href=\"#Creacion-de-Variables\" data-toc-modified-id=\"Creacion-de-Variables-4.12.1\"><span class=\"toc-item-num\">4.12.1&nbsp;&nbsp;</span>Creacion de Variables</a></span><ul class=\"toc-item\"><li><span><a href=\"#Crear-Variables-derivadas-de-Otras\" data-toc-modified-id=\"Crear-Variables-derivadas-de-Otras-4.12.1.1\"><span class=\"toc-item-num\">4.12.1.1&nbsp;&nbsp;</span>Crear Variables derivadas de Otras</a></span></li><li><span><a href=\"#Crear-Variables-de-Categorico-a-Numerico\" data-toc-modified-id=\"Crear-Variables-de-Categorico-a-Numerico-4.12.1.2\"><span class=\"toc-item-num\">4.12.1.2&nbsp;&nbsp;</span>Crear Variables de Categorico a Numerico</a></span></li></ul></li></ul></li><li><span><a href=\"#Reduccion-de-Dimensionalidad-y-Seleccion-de-Variables-(PCA)\" data-toc-modified-id=\"Reduccion-de-Dimensionalidad-y-Seleccion-de-Variables-(PCA)-4.13\"><span class=\"toc-item-num\">4.13&nbsp;&nbsp;</span>Reduccion de Dimensionalidad y Seleccion de Variables (PCA)</a></span></li><li><span><a href=\"#Balance-de-datos\" data-toc-modified-id=\"Balance-de-datos-4.14\"><span class=\"toc-item-num\">4.14&nbsp;&nbsp;</span>Balance de datos</a></span></li></ul></li><li><span><a href=\"#MODELAMIENTO-DE-LOS-DATOS-(MACHINE-LEARNING)\" data-toc-modified-id=\"MODELAMIENTO-DE-LOS-DATOS-(MACHINE-LEARNING)-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>MODELAMIENTO DE LOS DATOS (MACHINE LEARNING)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dividir-el-dataset-en-Training-set-y-Test-set\" data-toc-modified-id=\"Dividir-el-dataset-en-Training-set-y-Test-set-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Dividir el dataset en Training set y Test set</a></span></li></ul></li><li><span><a href=\"#Validacion-y-Evaluacion-Cruzada-(k-fold-Cross-Validation)\" data-toc-modified-id=\"Validacion-y-Evaluacion-Cruzada-(k-fold-Cross-Validation)-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Validacion y Evaluacion Cruzada (k-fold Cross Validation)</a></span></li><li><span><a href=\"#Optimizacion-de-Hiper-parametros-(Hyper-Parameter-optimization)\" data-toc-modified-id=\"Optimizacion-de-Hiper-parametros-(Hyper-Parameter-optimization)-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Optimizacion de Hiper parametros (Hyper Parameter optimization)</a></span></li><li><span><a href=\"#Evaluacion-final-del-modelo-con-el-Test-set\" data-toc-modified-id=\"Evaluacion-final-del-modelo-con-el-Test-set-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Evaluacion final del modelo con el Test set</a></span></li><li><span><a href=\"#Implementacion-del-Modelo-(Deploying)\" data-toc-modified-id=\"Implementacion-del-Modelo-(Deploying)-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Implementacion del Modelo (Deploying)</a></span></li><li><span><a href=\"#Comunicacion-de-Resultados-(Data-Story-Telling)\" data-toc-modified-id=\"Comunicacion-de-Resultados-(Data-Story-Telling)-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Comunicacion de Resultados (Data Story Telling)</a></span></li><li><span><a href=\"#Conclusiones\" data-toc-modified-id=\"Conclusiones-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Conclusiones</a></span></li><li><span><a href=\"#Ayudas-Y-Referencias\" data-toc-modified-id=\"Ayudas-Y-Referencias-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Ayudas Y Referencias</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo del Trabajo <a class=\"tocSkip\">\n",
    "El objetivo inicial del trabajo es utilizar Python para el procesamiento, descripcion  y visualización de datos, con el fin de dejar los datos preparados para ser usados con algoritmos de Machine Learning para Regresión o Clasificación como objetivo final del trabajo.\n",
    "\n",
    "El trabajo se realizara en Python usando un jupyter notebook o un Notebook de Google Collaboratory, llenando los campos de este archivo y subirlo a github.\n",
    "\n",
    "## Las actividades a realizar<a class=\"tocSkip\">\n",
    "    \n",
    "    \n",
    "1) Preparar los datos, hacer cada uno de los pasos solo si es necesario:\n",
    "      \n",
    "    - Poner los datos en sus tipos de datos correctos\n",
    "    - Remover los valores duplicados\n",
    "    - Procesar y Reemplazar los datos que faltan (Missing N.A values)\n",
    "    - Manejo de outliers\n",
    "    - Re-escalar las variables\n",
    "    - Normalizar\n",
    "    - Data Binning\n",
    "        - Conversión Numérico a categórico\n",
    "        - Conversión Categórico a Numérico\n",
    "\n",
    "2) Realizar una exploración estadística y con visualización de los datos\n",
    "    - Scatter plots\n",
    "    - Histogramas\n",
    "    - boxplot, o las gráficas que considere necesarias\n",
    "\n",
    "\n",
    "3) Entrene y Evalue modelos de Machine Learning según sea su problema, recuerde usar k-fold cross-validation para evitar over fitting, algunos algoritmos recomendados\n",
    "\n",
    "    (a) PREDICCIÓN:\n",
    "    \n",
    "            ▪ Regresión Lineal\n",
    "            ▪ Lasso\n",
    "            ▪ Ridge\n",
    "            ▪ Decision Tree regressor\n",
    "            ▪ Random Forest regressor\n",
    "            ▪ SVR\n",
    "            ▪ KNR\n",
    "            ▪ catboost regressor\n",
    "            \n",
    "    \n",
    "    \n",
    "    (b) CLASIFICACION\n",
    "      \n",
    "            ▪ Regresión Logística\n",
    "            ▪ LinearSVC\n",
    "            ▪ KernelSVC\n",
    "            ▪ Decision Tree clasifier\n",
    "            ▪ Random Forest classifier\n",
    "            ▪ K-NN\n",
    "            ▪ GaussianNB\n",
    "            ▪ catboost\n",
    "    \n",
    "**Nota: utilizar visualizaciones para comunicar los resultados obtenidos.**    \n",
    "            \n",
    "4) Realizar hyperparameter optimization  de los dos métodos que tengan mejor resultado en los pasos anteriores para mejorar el desempeño obtenido. (Tenga en cuenta, que realizar el hyperparameter tuning no garantiza una mejora significativa en el desempeño, pero es bueno intentarlo)\n",
    "    \n",
    "5) Realizar comentarios de cada paso que va realizando en el documento y finalmente también agregue **conclusiones** de:\n",
    "    \n",
    "    - los datos procesados\n",
    "    - la información que observa\n",
    "    - Comparación de la evaluación de los Métodos\n",
    "    - Cual fue el mejor modelo que obtuvo luego de la hiper parametrización\n",
    "    \n",
    "    \n",
    "*NOTA: No dude en contactarme para cualquier pregunta o inquietud :)\n",
    "\n",
    "## FORMATO DE ENTREGA  <a class=\"tocSkip\">\n",
    "\n",
    "EL jupyter notebook estara alojado en un repositorio de Github\n",
    "\n",
    "El código debe tener comentarios y explicaciones de la solución del trabajo.\n",
    "\n",
    "\n",
    "## EVALUACION <a class=\"tocSkip\">\n",
    "(33 % ) preparacion, descripcion y visualizacion de datos\n",
    "    \n",
    "(33 % ) Machine Learning, hiperparametrizacion y Conclusiones\n",
    "    \n",
    "\n",
    "|Porcentaje en la evaluación | Descripción| Nada | Incompleto | Completo |\n",
    "| :---: |:---: |:---: |:---: |:---: |\n",
    "| 11 % |**Preparación de los datos** <br> (datos nulos, outliers, duplicados, <br> corrección de formatos de tipos de datos) |\n",
    "|11 % | **Visualización de datos** <br> (Univarible y Bivariable) <br> Histogramas, boxplots, correlaciones, etc|\n",
    "|11 % | **Descripción y Análisis Estadístico de los datos** |\n",
    "|11 % | **Machine Learning** <br> Entrenar y evaluar todos los modelos propuestos |\n",
    "|11 % | **Hiper parametrizacion** <br> Hiperparametrizar 2 modelos y escoger el mejor modelo |   \n",
    "|11 % | **Resultados y Conclusiones** <br> analisis de los datos, conclusiones del modelos y los resultados obtenidos|\n",
    "\n",
    "\n",
    "Ejemplos y links de ayuda al final del documento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XE2q_0ARHJoM"
   },
   "source": [
    "# Definir el Problema a Resolver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado un dataset basado en los precios de las casas en Bostón, se busca determinar el valor de la misma de acuerdo a diferentes características que se almacenan en el dataset como lo son la tasa de criminalidad, la proporción de zona residencial en viviendas de más de 25.000 sq.ff. y entre otras variables que permitirán aproximar el valor de cada vivienda. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0fWGXusiHJoO"
   },
   "source": [
    "## Describir los datos de entrada y salida\n",
    "\n",
    "Se cuenta con un total de 17 variables en el dataset.\n",
    "\n",
    "**Variables de entrada**\n",
    "\n",
    "Variable | Descripción  | \n",
    " :---:   | :---: | \n",
    "zn       | Proporción de terrenos residenciales para lotes de más de 25.000 sq.ff.\n",
    "black    | 1000 (Bk - 0,63) 2 donde Bk es la proporción de negros por ciudad. \n",
    "rad      | Índice de accesibilidad a las autopistas radiales.\n",
    "indus    | Proporción de hectáreas comerciales no minoristas por ciudad.\n",
    "ptratio  | Proporción alumnos-profesores por ciudad.\n",
    "lstat    | Menor estatus de la población (en porcentaje).\n",
    "nbgde    | **No se tiene concocimiento de esta variable**\n",
    "chas     | Variable ficticia del río Charles (1 si el tramo limita con el río, 0 en caso contrario).\n",
    "ftyrgv   | **No se tiene concocimiento de esta variable**\n",
    "ID       | Identificador de cada casa.\n",
    "crim     | Tasa de delincuencia per cápita por ciudad.\n",
    "dis      | Media ponderada de las distancias a cinco centros de empleo de Boston.\n",
    "tax      | Tasa total de impuesto a la propiedad por 10.000 dólares.\n",
    "rm       | Número promedio de habitaciones por vivienda.\n",
    "age      | Proporción de unidades ocupadas por sus propietarios construidas antes de 1940.\n",
    "nox      | Concentración de óxidos de nitrógeno (partes por cada 10 millones).\n",
    "\n",
    "**Variable de salida**\n",
    "\n",
    "Variable | Descripción  | \n",
    " :---:   | :---: | \n",
    "medv     | Valor promedio de las viviendas ocupadas por sus propietarios en $1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MjPWhesSHJoO"
   },
   "source": [
    "# Importar Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ksQYJRZTHJoP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LM4dC7lBHJoT"
   },
   "source": [
    "# Cargar Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R2C-zrtBHJoT"
   },
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv(\"../data/raw/Precios_Casas_BostonDS.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descripcion General del Dataset\n",
    "- numero de filas y columnas\n",
    "- tipos de datos y si estan correctos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cantidad de datos inicial:\n",
    "- 13440 filas, 17 Columns , memory usage: 1.9+ MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de calidad de datos general\n",
    "- Filas con valores exactamente iguales (duplicados)\n",
    "- Columnas duplicadas\n",
    "- Columnas con valores constantes o sin informacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se verifica existencia de valores duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminación de columnas innecesarias "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se eliminan variables existentes en el dataset que no brindan información y se desconoce la relación directa con la variable de salida (medv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.drop(['ftyrgv','Unnamed: 0', 'nbgde','ID','index'], axis= 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reemplazo de valores faltantes\n",
    "- Para más facilidad se decide reemplazar los valores faltantes (NaN) dentro del data set por el caracter '?' ya que esto nos permite dividir los datos en el siguiente paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.replace(np.nan, '?', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se verifica la existencia de valores nulos en el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividir el dataset en Training set y Test set\n",
    "\n",
    "Este paso debe ser al inicio de los proyectos,  Se deben realizar todas las transformaciones, preparación de datos y limpieza de los datos, en el train set y en la evaluacion se deben aplicarl al test set y a los datos nuevos que lleguen al sistema. Esta division inicial se hace para evitar data leakage de los datos de test a los datos de train, por ejemplo en las imputaciones.\n",
    "\n",
    "Por este motivo se realizara en esta parte.\n",
    "\n",
    "Division de los datos de entrenamiento (Train set) y de Evaluacion (test - set), las divisiones utilizadas son\n",
    "\n",
    "- 80% (train) , 20%(test)\n",
    "- 70% (train) , 30%(test)\n",
    "\n",
    "El **Train set** se hace para seleccion de Modelos y el **Test-set** solamente para la evaluacion Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_raw.drop('medv',axis=1)\n",
    "y = data_raw['medv']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2,random_state = 123, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tamaño del set de entrenamiento', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([X_train,y_train],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5gnmm4vdHJoV"
   },
   "source": [
    "# Descripcion  y Limpieza de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se describen nuevamente los datos seleccionados para el entrenamiento (80%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZYzna68bHJoW"
   },
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se obtienen los valores únicos por columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_train:\n",
    "    print(f\"{col} = {X_train[col].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de resultados\n",
    "- Todas las columnas contienen los valores ['xxxxx', 'gdhg7u8whui 784gryb', '-26543765432345.0', '88876599956788.0', '?'] que configuran un ruido para el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se reemplazan los valores desconocidos por el valor NaN para luego poderlos eliminar mediante la función 'dropna'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.replace(['xxxxx', '-26543765432345.0', 'gdhg7u8whui 784gryb', '88876599956788.0'], np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restablecer los valores nulos del dataset de entrenamiento\n",
    "Se recuperan los valores NaN que anteriormente se habían cambiado por el caracter '?' en el paso de particionamiento del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.replace('?', np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análisis previo del set de entrenamiento: \n",
    "- Filas: 4048\n",
    "- Columnas: 13\n",
    "- Memoria 411.2+ KB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perfilamiento del set de entrenamiento - reporte 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** recordar que los datos NaN en el dataset original se cambiaron por el caracter '?' para poder realizar el particionamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(X_train, title=\"Boston House Prices Report 1\",\n",
    "                        explorative=True, samples=None, correlations=None, \n",
    "                        missing_diagrams=None, duplicates=None, interactions=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_file(\"../reports/Boston_house_prices_report_1.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis de perfil generado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable | Análisis  | \n",
    " :---:   | :---: | \n",
    "zn       | Para el 69.6% de datos la proporción de terrenos residenciales para lotes de más de 25.000 m² es igual a 0.\n",
    "black    | Variable con ALTA CARDINALIDAD, 237 valores diferentes, 396.9 20.5% más común, el 55.6% de los valores se reconoce como (other).\n",
    "rad      | Variable balanceada, 9% de valores faltantes, 24.0 25.6% más común.\n",
    "indus    | Variable con ALTA CARDINALIDAD, 68 valores diferentes, 18.1 25.8% más común, el 15.9% de los valores se reconoce como (other).\n",
    "ptratio  | Variable con el 8.3% de valores faltantes, 20.2 26.5% más común.\n",
    "lstat    | Variable con ALTA CARDINALIDAD, 310 valores diferentes, el 74% de los valores se reconoce como (other).\n",
    "chas     | Variable desbalanceada, 7.6% de valores faltantes, el 86.9% de casas no limita con el río.\n",
    "crim     | Variable con ALTA CARDINALIDAD, 332 valores diferentes, 9.6% de valores faltantes, el 75.9% se reconoce como (other).\n",
    "dis      | Variable con ALTA CARDINALIDAD, 295 valores diferentes, 10.5% de valores faltantes, el 72.4% se reconoce como (other).\n",
    "tax      | Variable con ALTA CARDINALIDAD, 59 valores diferentes, 666.0 25.4% más común. \n",
    "rm       | Variable con ALTA CARDINALIDAD, 308 valores diferentes, 8.8% de valores faltantes, el 70% de los valores se reconoce como (other).\n",
    "age      | Variable con ALTA CARDINALIDAD, 260 valores diferentes, 100 8.8% más común, el 63.2% de los valores se reconoce como (other).\n",
    "nox      | Variable con ALTA CARDINALIDAD, 77 valores diferentes, 605 4.9% más común, 7.4% de valores faltantes.\n",
    "\n",
    "**Warnings** <br>\n",
    "\n",
    "El dataset tiene 3280(81%) de filas duplicadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cantidad de valores nulos existentes en el set de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train.isna().sum()*100/len(X_train)).plot.bar(title = 'Percentage of Missing Values per column',color=X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porcentaje de valores nulos en el set de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = X_train.isna().sum()\n",
    "missing[missing>0]*100/len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis General Univariable y Bivariable \n",
    "Analisis de cada una de las variables para lograr calidad de datos en cada columna\n",
    "- **Correccion del tipo de dato (numericas, categoricas, string) de cada columna (optimizar memoria)**\n",
    "- Deteccion de numero de datos faltantes\n",
    "- Deteccion de duplicados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Es necesario hacer un dataset auxiliar para allí eliminarlos temporalmente y poder obtener las medidas de tendencia central (moda, mediana y media) necesarias para la imputación de datos faltantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columnTreatment(column, data):\n",
    "    dataColumn = data[column].dropna()\n",
    "    dataColumn = dataColumn.astype('float64')\n",
    "\n",
    "    dataFrame = pd.DataFrame(dataColumn)\n",
    "    dataFrame.reset_index(inplace = True, drop = True)\n",
    "\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getQuartiles(dataFrame, column):\n",
    "    Q1 = np.percentile(dataFrame[column].dropna(), 25, interpolation = 'midpoint')\n",
    "    Q3 = np.percentile(dataFrame[column].dropna(), 75, interpolation = 'midpoint')\n",
    "\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    return {'Q1': Q1,'Q3': Q3, 'IQR': IQR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBounds(dataFrame, column, quartiles):\n",
    "    Q1 = quartiles['Q1']\n",
    "    Q3 = quartiles['Q3']\n",
    "    IQR = quartiles['IQR']\n",
    "\n",
    "    print('Q1', Q1)\n",
    "    print('Q3', Q3)\n",
    "    print('IQR', IQR)\n",
    "\n",
    "    # Upper bound\n",
    "    upper = np.where(dataFrame[column] >= (Q3 + 1.5*IQR))\n",
    "    print(\"Upper bound:\",upper)\n",
    "\n",
    "    # Below Lower bound\n",
    "    lower = np.where(dataFrame[column] <= (Q1 - 1.5*IQR))\n",
    "    print(\"Lower bound:\", lower)\n",
    "\n",
    "    return {'upper': upper, 'lower': lower}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteOutliersDataframe(dataFrame, bounds):\n",
    "    upper = bounds['upper']\n",
    "    lower = bounds['lower']\n",
    "\n",
    "    dataFrame.drop(upper[0], inplace = True)\n",
    "    dataFrame.drop(lower[0], inplace = True)\n",
    "    dataFrame.reset_index(inplace = True, drop = True)\n",
    "\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteOutliersSetTrain(train, column):\n",
    "    quartiles = getQuartiles(train.copy(), column)\n",
    "    bounds = getBounds(train.copy(), column, quartiles)\n",
    "    train = deleteOutliersDataframe(train.copy(), bounds)\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDataframe(train, column):\n",
    "    dataFrame = columnTreatment(column, train.copy())\n",
    "    quartiles = getQuartiles(dataFrame.copy(), column)\n",
    "    bounds = getBounds(dataFrame.copy(), column, quartiles)\n",
    "\n",
    "    dataFrame = deleteOutliersDataframe(dataFrame.copy(), bounds)\n",
    "\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis varible CRIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['crim'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se evidencia que la variable \"crim\" contiene 388 datos nulos, asi que vamos a proceder a ver la distribucion de los datos para analizar que se puede hacer con los datos faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crim = train['crim'].dropna().astype('float64')\n",
    "px.box(crim,title = \"zn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crim.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La distribucion de los datos se ve muy contaminada por los valores atipicos entonces vamos a proceder a eliminarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCrim = generateDataframe(train.copy(), 'crim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(dfCrim,title = \"zn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A este punto aun conservamos algunos valores atipicos, pero a diferencia de la versión inicial esta ya esta mas depurada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanCrim = dfCrim['crim'].mean()\n",
    "print(meanCrim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(dfCrim,x=\"crim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rellenamos los datos que estan como nulos por el valor de la media esto según se ve en la histograma no altera la distribución de forma significativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['crim'].replace(np.nan,meanCrim,inplace=True)\n",
    "train['crim'] = train['crim'].astype('float64')\n",
    "train['crim'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de la imputación debemos recordar que los datos atipicos se quitaron para encontrar la media, entonces en esta ocacion debemos quitar los datos atipicos de nuestro set de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(inplace = True, drop = True)\n",
    "train = deleteOutliersSetTrain(train,'crim')\n",
    "train['crim'] = train['crim'].astype('float64')\n",
    "train['crim'].describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(train['crim'],x=\"crim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis variable ZN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zn = train['zn'].dropna().astype('float64')\n",
    "px.box(zn,title = \"zn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(zn,x=\"zn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['zn'].describe()\n",
    "print(train['zn'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['zn'] = train['zn'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['zn'].value_counts()[0]*100/len(train['zn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo en cuenta que el 66 % de los datos son ceros los datos que estan como null se van a llenar con los datos de la moda en este caso 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['zn'].fillna(train['zn'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['zn'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis variable INDUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indus = train['indus'].dropna().astype('float64')\n",
    "indus.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(indus,title = \"indus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos par este caso no tiene outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(indus,x=\"indus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['indus'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por como esta la distribución vamos a descargar los datos nullos ya que para hacer una imputacion se deberian tener muchas factores en cuenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(train[train['indus'].notnull() == False].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['indus'] = train['indus'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['indus'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['indus'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis variable CHAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chas = train['chas'].dropna().astype('float64')\n",
    "print(train['chas'].value_counts())\n",
    "px.histogram(chas,x=\"chas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['chas'] = train['chas'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['chas'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['chas'].dropna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo en cuenta esta distribución vamos a imputar los datos faltantes como 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['chas'].fillna(train['chas'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['chas'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis variable NOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noxx = train['nox'].dropna().astype('float64')\n",
    "noxx.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['nox'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(noxx,title = \"nox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(noxx,x=\"nox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(inplace = True, drop = True)\n",
    "train['nox'] = train['nox'].astype('float64')\n",
    "train = deleteOutliersSetTrain(train,'nox')\n",
    "train['nox'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(train['nox'],title = \"nox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(train['nox'],x=\"nox\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par este caso tambien vamos a descartar los valores que estanc como null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(train[train['nox'].notnull() == False].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['nox'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis variable RM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm = train['rm'].dropna().astype('float64')\n",
    "rm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(rm,title = \"rm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el Boxplot se evidencia que esta variable tiene valores atipicos entonces se procede a quitarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(inplace = True, drop = True)\n",
    "train['rm'] = train['rm'].astype('float64')\n",
    "train = deleteOutliersSetTrain(train,'rm')\n",
    "train['rm'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(train['rm'],title = \"rm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['rm'] = train['rm'].astype('float64')\n",
    "train['rm'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(train['rm'],x=\"rm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando la distribución vamos a descargar los datos que tenemos como null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(train[train['rm'].notnull() == False].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['rm'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis varible AGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = train['age'].dropna().astype('float64')\n",
    "age.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(age,title = \"rm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(age,x=\"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['age'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando la distribucion y la cantidad de datos que son que son null vamos a proceder a eliminar los datos null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(train[train['age'].notnull() == False].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['age'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis variable DIS - ALTA CARDINALIDAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se verifica la cantidad de valores faltantes en la columna DIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['dis'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se debe generar un dataframe auxiliar para así eliminar temporalmetne los valores NaN y encontrar la media que en este caso nos servirá para imputar los datos faltantes más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeDis = generateDataframe(train.copy(), 'dis')\n",
    "meanDis = dataframeDis['dis'].mean()\n",
    "print('Mean found:' , meanDis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columna DIS en el dataset de entrenamiento con valores NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['dis'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columna DIS en el dataset de entrenamiento con valores imputados dada la media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['dis'].fillna(meanDis, inplace = True)\n",
    "train['dis'] = train['dis'].astype('float64')\n",
    "train['dis'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(train['dis'], title = \"BOXPLOT DIAGRAM OF DIS VARIABLE WITH OUTLIERS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se borran los outliers en el dataset de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(inplace = True, drop = True)\n",
    "train = deleteOutliersSetTrain(train.copy(), 'dis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(train['dis'], title = \"BOXPLOT DIAGRAM OF DIS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se verifica la cantidad de valores faltantes para la variable DIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['dis'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis variable RAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se verifica la cantidad de valores faltantes en la columna rad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['rad'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtiene el listado de valores únicos que contiene la columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colum = train['rad'].unique()\n",
    "colum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráfico de barras para observar la distribución de la variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = train['rad'].value_counts()\n",
    "bar.plot.bar(title=\"RAD DISTRIBUTION\", color=bar.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mayor índice de accesibilidad a las autopistas es 24.0, así que se imputan los datos faltantes tomando la moda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['rad'].fillna(train['rad'].mode()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se verifica la cantidad de valores faltantes para la variable RAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['rad'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis variable TAX - ALTA CARDINALIDAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se verifica la cantidad de valores faltantes en la columna tax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tax'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráfico de barras para observar la distribución de la variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = train['tax'].value_counts()\n",
    "bar.plot.bar(title=\"TAX DISTRIBUTION\", color=bar.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(train['tax'], title = \"BOXPLOT DIAGRAM OF TAX VARIABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El valor que más se repite en la columna TAX es 666.0, así que se imputan los datos faltantes tomando la moda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tax'].fillna(train['tax'].mode()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se verifica la cantidad de valores faltantes para la variable TAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tax'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis variable PTRATIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se verifica la cantidad de valores faltantes en la columna ptratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['ptratio'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtiene el listado de valores únicos que contiene la columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colum = train['ptratio'].unique()\n",
    "colum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráfico de barras para observar la distribución de la variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = train['ptratio'].value_counts()\n",
    "bar.plot.bar(title=\"PTRATIO DISTRIBUTION\", color=bar.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(train['ptratio'], title = \"BOXPLOT DIAGRAM OF PTRATIO VARIABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El valor que más se repite en la columna PTRATIO es 20.2, así que se imputan los datos faltantes tomando la moda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['ptratio'].fillna(train['ptratio'].mode()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se verifica la cantidad de valores faltantes para la variable PTRATIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['ptratio'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis variable BLACK - ALTA CARDINALIDAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se verifica la cantidad de valores faltantes en la columna black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['black'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráfico de barras para observar la distribución de la variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = train['black'].value_counts()\n",
    "bar.plot.bar(title=\"BLACK DISTRIBUTION\", color=bar.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(train['black'], title = \"BOXPLOT DIAGRAM OF BLACK VARIABLE WITH OUTLIERS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se debe generar un dataframe auxiliar para así eliminar temporalmente los valores NaN y encontrar la moda que en este caso nos servirá para imputar los datos faltantes más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeBlack = generateDataframe(train.copy(), 'black')\n",
    "modeBlack = dataframeBlack['black'].mode()[0]\n",
    "print('Mode found:' , modeBlack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columna BLACK en el dataset de entrenamiento con valores NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['black'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columna BLACK en el dataset de entrenamiento con valores imputados dada la moda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['black'].fillna(modeBlack, inplace = True)\n",
    "train['black'] = train['black'].astype('float64')\n",
    "train['black'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se borran los outliers en el dataset de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(inplace = True, drop = True)\n",
    "train = deleteOutliersSetTrain(train.copy(), 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se verifica la cantidad de valores faltantes para la variable BLACK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['black'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis variable LSTAT - ALTA CARDINALIDAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se verifica la cantidad de valores faltantes en la columna LSTAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['lstat'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráfico de barras para observar la distribución de la variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = train['lstat'].value_counts()\n",
    "bar.plot.bar(title=\"LSTAT DISTRIBUTION\", color=bar.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(train['lstat'], title = \"BOXPLOT DIAGRAM OF LSTAT VARIABLE WITH OUTLIERS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se debe generar un dataframe auxiliar para así eliminar temporalmente los valores NaN y encontrar la moda que en este caso nos servirá para imputar los datos faltantes más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeLstat = generateDataframe(train.copy(), 'lstat')\n",
    "modeLstat = dataframeLstat['lstat'].mode()[0]\n",
    "print('Mode found:' , modeLstat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columna LSTAT en el dataset de entrenamiento con valores NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['lstat'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columna LSTAT en el dataset de entrenamiento con valores imputados dada la moda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['lstat'].fillna(modeLstat, inplace = True)\n",
    "train['lstat'] = train['lstat'].astype('float64')\n",
    "train['lstat'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se borran los outliers en el dataset de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(inplace = True, drop = True)\n",
    "train = deleteOutliersSetTrain(train.copy(), 'lstat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(train['lstat'], title = \"BOXPLOT DIAGRAM OF LSTAT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se verifica la cantidad de valores faltantes para la variable LSTAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['lstat'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "que3vMzKnhwJ"
   },
   "source": [
    "## Remover Datos Duplicados Exactos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.drop_duplicates(keep = False, inplace = True, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perfilamiento del set de entrenamiento - reporte 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile = ProfileReport(X_train, title=\"Boston House Prices Report 2\",\n",
    "#                         explorative=True, samples=None, correlations=None, \n",
    "#                         missing_diagrams=None, duplicates=None, interactions=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile.to_notebook_iframe()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile.to_file(\"../reports/Boston_house_prices_report_2.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AFSKQY4Amzwv"
   },
   "source": [
    "## Identificacion de Variables\n",
    "- Variables de entrada y de salida\n",
    "- Tipo de Variables (categoricas o Numericas)\n",
    "- Tipo de datos (int, float, string, factor, boolean, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bxoNbnANoJt8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UegcIxItnhwM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2LmnrQtbHJox"
   },
   "source": [
    "## Procesamiento de Datos Faltantes\n",
    "Las opciones que se pueden usar dependiendo del analisis de los datos son:\n",
    "### Borrar Filas\n",
    "- Borrar las filas que les falten todos los datos\n",
    "- Borrar Solo las filas que les falta las variables mas importantes o la salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zFwqgwOhHJox"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1S_iRK8rHJo0"
   },
   "source": [
    "### Reemplazar datos faltantes con la Media/ Moda/ Mediana (Mean/ Mode/ Median Imputation)\n",
    "De necesitarse hacer imputacion se debe verificar los outliers para no cometer errores en la imputacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kIIWI1KCHJo0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "68OZ-hJKHJo8"
   },
   "source": [
    "## Remover Datos Duplicados Exactos\n",
    "\n",
    "Hacer al principio luego de cargar los datos y cada vez luego de Hacer imputaciones, eliminacion de columnas o registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6cCf27oSHJpB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GnkcN0goHJod"
   },
   "source": [
    "## Analisis Univariable\n",
    "\n",
    "Estadistico Descriptico y Analisis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LEvH4-LGHJod"
   },
   "source": [
    "### Variables Numericas\n",
    "\n",
    "Tendencia Central  | Medida de Dispersión  |  Visualizacion\n",
    " :---: | :---: | :---:\n",
    "Media   | Rango      |  Histogramas\n",
    "Mediana | Cuartiles  |  Boxplots\n",
    "Moda    | Rango inter cuartil (IQR)|  \n",
    "Minimo  | Varianza   |  \n",
    "Maximo  | Desviacion Estandard | \n",
    "   .    | Skewness   |  \n",
    " .      | Kurtosis   |  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CDTBsGi0HJoe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mXQHmtMbHJoh"
   },
   "source": [
    "### Variables Categoricas\n",
    "- Numero de elementos por categoria\n",
    "- Porcentaje de elementos por categoria\n",
    "- Graficos de barras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hPY0sHqNHJoi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jmf_f9TOHJok"
   },
   "source": [
    "## Analisis Bivariable\n",
    "Estadistico Descriptico y Analisis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3aVUkwJVHJom"
   },
   "source": [
    "### Numericas vs Numericas\n",
    "- Scatter Plot\n",
    "- Heatmap\n",
    "- Correlacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "067r3JdGHJon"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9SfZHh6oHJor"
   },
   "source": [
    "### Categoricas vs Categoricas\n",
    "\n",
    "- Two-way table\n",
    "- Graficas de Barras apiladas\n",
    "- Chi-Square Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T544jyPsHJos"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oQKXaEcqHJou"
   },
   "source": [
    "### Categoricas vs Numericas\n",
    "- Z-Test/ T-Test\n",
    "- ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F8coJJA6HJov"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r2X6dK04HJpD"
   },
   "source": [
    "## Procesamiento de Outliers\n",
    "### Deteccion de Outliers (Univariables y Bi variables)\n",
    "- Boxplots\n",
    "- Scatter Plots\n",
    "- Histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bc5mYM_qHJpE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aiDgzB32HJpI"
   },
   "source": [
    "### Remover Outliers\n",
    "las opciones son:\n",
    "- Borrar los datos atipicos (outliers) \n",
    "- Transformar la variable (Ej: escalar, cambiar a esccala Log o a Escala lineal)\n",
    "- Reemplazar los outliers conla Media/ Moda/ Mediana (Mean/ Mode/ Median Imputation)\n",
    "- Reemplazar valores con modelo predictivo\n",
    "- Separarlos y analizarlos aparte (si son muchos outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DsDqeIPbHJpI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r2X6dK04HJpD"
   },
   "source": [
    "## Procesamiento de Outliers\n",
    "### Deteccion de Outliers (Univariables y Bi variables)\n",
    "- Boxplots\n",
    "- Scatter Plots\n",
    "- Histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bc5mYM_qHJpE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aiDgzB32HJpI"
   },
   "source": [
    "### Remover Outliers\n",
    "las opciones son:\n",
    "- Borrar los datos atipicos (outliers) \n",
    "- Transformar la variable (Ej: escalar, cambiar a esccala Log o a Escala lineal)\n",
    "- Reemplazar los outliers conla Media/ Moda/ Mediana (Mean/ Mode/ Median Imputation)\n",
    "- Reemplazar valores con modelo predictivo\n",
    "- Separarlos y analizarlos aparte (si son muchos outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DsDqeIPbHJpI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4Fn2YIvHJpL"
   },
   "source": [
    "## Feature Engineering \n",
    "### Transformacion de Variables\n",
    "**Se usa cuando es necesario:**\n",
    "- Cambiar la escala las variables (Normalizar, escalamiento, etc), esto no cambia la forma de la distribucion de los datos\n",
    "- Cambiar relaciones No ineales en Lineales (Ej: cambiar la escala Logaritmica a lineal)\n",
    "- Usar una la distribución simétrica en vez de una distribución sesgada o asimetrica, Para una la distribución sesgada a la derecha, tomamos la raíz cuadrada / cúbica o el logaritmo de la variable, y para la desviación a la izquierda, tomamos el cuadrado / cubo o exponencial de las variables.\n",
    "- Cambair variables continuas a categoricas\n",
    "\n",
    "Algunas transformaciones son:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "he4lxsEUHJpM"
   },
   "source": [
    "#### Normalizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7giKrRY8HJpM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h0FKIrJHHJpO"
   },
   "source": [
    "#### Escalamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hzGcwGaWHJpP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jYPdefrhHJpS"
   },
   "source": [
    "#### Logaritmica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wU-lW1AQHJpS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zUundEM6HJpT"
   },
   "source": [
    "#### raíz cuadrada / cúbica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5woteVvBHJpU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2GNrgJzHHJpW"
   },
   "source": [
    "#### Binning , Cambios de Numericas a Categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cS5y1OPSHJpW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6PpnSEDHJpY"
   },
   "source": [
    "## Analisis Univariable y Bivariable Final\n",
    "Luego de Realizar:\n",
    "- Tratamiento de datos Nulos\n",
    "- Tratamiento de outliers\n",
    "- Transformacion de Variables\n",
    "\n",
    "Es necesario realizar nuevamente analisis univarible y bivariable para identificar como cambiaron nuestros datos y para verificar si estan listos para usarse para crear un modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uRwep7LjHJpZ"
   },
   "source": [
    "### Creacion de Variables\n",
    "pueden ser:\n",
    "#### Crear Variables derivadas de Otras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ywBVDG8vHJpZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JCcM4XFbHJpd"
   },
   "source": [
    "#### Crear Variables de Categorico a Numerico\n",
    "Puede ser\n",
    "- Definir un numero a cada categoria\n",
    "- One hot encoder (dummy columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ubILpEdWHJpi"
   },
   "source": [
    "## Reduccion de Dimensionalidad y Seleccion de Variables (PCA)\n",
    "\n",
    "De ser necesario si tiene muchas variables o sospecha que aun tiene variables que no aportan mucho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S2JIcRk-HJpi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oEu3sW98HJpk"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lw3hG0rCHJpl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELAMIENTO DE LOS DATOS (MACHINE LEARNING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validacion y Evaluacion Cruzada (k-fold Cross Validation)\n",
    "\n",
    "Se hace seleccion de los mejores modelos usando el Training Set y k-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizacion de Hiper parametros (Hyper Parameter optimization)\n",
    "\n",
    "Se seleccionan solo los mejores modelos para realizar el ajuste de hiperparametros, ya que tiene una carga computacional alta.\n",
    "\n",
    "Al final se obtienen los parametros del mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluacion final del modelo con el Test set\n",
    "\n",
    "Tomar los parametros obtenidos en el paso anterior, se crea el modelo con esos pararmetros y se entrena el modelo con todos los datos del **Train -set**\n",
    "\n",
    "Finalmente se realiza la evaluacion (segun su problema si es de regresion o de clasificacion) usando el **Test - set** para definir si el modelo obtenido esta bien. Compare los resultados con el **Train -set** vs los resultados con el **Test - set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementacion del Modelo (Deploying)\n",
    "Con el análisis básico y el ajuste hecho, comienza el trabajo real (ingeniería).\n",
    "\n",
    "El último paso para poner en produccion el modelo de prediccion sera:\n",
    "1. Entrenarlo en todo el conjunto de datos nuevamente, para hacer un uso completo de todos los datos disponibles. \n",
    "2. Usar los mejores parámetros encontrados mediante la validación cruzada, por supuesto. Esto es muy similar a lo que hicimos al principio, pero esta vez teniendo una idea de su comportamiento y estabilidad. La evaluación se realizó con honestidad, en divisiones distintas de entrenamiento / prueba.\n",
    "\n",
    "El predictor final se puede serializar y grabar en el disco, de modo que la próxima vez que lo usemos, podemos omitir todo el entrenamiento y usar el modelo capacitado directamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle # Esta es una libreria de serializacion nativa de python, puede tener problemas de seguridad\n",
    "from joblib import dump # libreria de serializacion\n",
    "\n",
    "# garbar el modelo en un archivo\n",
    "#dump(Modelo_final, 'Nombre_Archivo_Modelo.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comunicacion de Resultados (Data Story Telling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ayudas Y Referencias\n",
    "\n",
    "- https://medium.com/@joserzapata/paso-a-paso-en-un-proyecto-machine-learning-bcdd0939d387\n",
    "- [Proyecto de Principio a Final sobre readmision de pacientes con Diabetes](https://github.com/JoseRZapata/Readmission-ML-Project)\n",
    "\n",
    "- [a-complete-machine-learning-walk-through-in-python-part-one](https://towardsdatascience.com/a-complete-machine-learning-walk-through-in-python-part-one-c62152f39420)\n",
    "\n",
    "\n",
    "- [a-starter-pack-to-exploratory-data-analysis-with-python-pandas-seaborn-and-scikit-learn](https://towardsdatascience.com/a-starter-pack-to-exploratory-data-analysis-with-python-pandas-seaborn-and-scikit-learn-a77889485baf#249d)\n",
    "\n",
    "- [a-data-science-for-good-machine-learning-project-walk-through-in-python-part-one](https://towardsdatascience.com/a-data-science-for-good-machine-learning-project-walk-through-in-python-part-one-1977dd701dbc)\n",
    "\n",
    "- [Ejemplos de Kaggle](https://www.kaggle.com/kernels?sortBy=hotness&group=everyone&pageSize=20&language=Python&kernelType=Notebook)\n",
    "\n",
    "- [END to END ML from data colletion to deployment](https://medium.com/datadriveninvestor/end-to-end-machine-learning-from-data-collection-to-deployment-ce74f51ca203)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docente: [Jose R. Zapata](https://joserzapata.github.io)\n",
    "- https://joserzapata.github.io\n",
    "- https://twitter.com/joserzapata\n",
    "- https://www.linkedin.com/in/jose-ricardo-zapata-gonzalez/   "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "mXQHmtMbHJoh",
    "3aVUkwJVHJom",
    "9SfZHh6oHJor",
    "1S_iRK8rHJo0",
    "Ey5Lk8evHJo3",
    "0Y0UV-GVHJo6",
    "he4lxsEUHJpM",
    "h0FKIrJHHJpO",
    "jYPdefrhHJpS",
    "zUundEM6HJpT",
    "2GNrgJzHHJpW",
    "uRwep7LjHJpZ",
    "JCcM4XFbHJpd"
   ],
   "name": "Trabajo_Preparacion_Datos_JoseR_Zapata.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "es",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "es",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Contenido",
   "title_sidebar": "Contenido",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "1855c35e3f7f09f8484660d0385aa8d0bfd61e96b634da27bfe8e51364b36feb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
